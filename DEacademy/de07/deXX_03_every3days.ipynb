{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "from airflow.exceptions import AirflowException\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# function to retrieve rocket launches data & store in s3\n",
    "def fetch_events(api_url, s3_conn_id, bucket_name, object_key):\n",
    "    resp = requests.get(api_url)\n",
    "    if resp.status_code!=200:\n",
    "        raise AirflowException(f\"user event api call fail, the resp status[{resp.status_code}]\")\n",
    "    # create s3 connection\n",
    "    # 透過Connection來取得S3Hook物件的實例\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # write rocket launch data to s3\n",
    "    with tempfile.NamedTemporaryFile('wb+') as fp:\n",
    "        temp_filename = fp.name  # 暫存檔案名\n",
    "        try:\n",
    "            fp.write(resp.content)\n",
    "            fp.flush()\n",
    "            s3_hook.load_file(filename=temp_filename,\n",
    "                                     bucket_name=bucket_name,\n",
    "                                     key=object_key,\n",
    "                                     replace=True)\n",
    "            print(f'upload user event data to s3: [{bucket_name}] -> {object_key}, success!')\n",
    "        except Exception as e:\n",
    "            print(f\"upload user event data to s3 fail:{e}\")\n",
    "\n",
    "\n",
    "# function to retrieve user event data & calculate stats\n",
    "def calculate_stats(s3_conn_id, bucket_name, data_object_key, result_object_key):\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # download rocket launches data form s3\n",
    "    try:\n",
    "        # 暫存檔案名\n",
    "        temp_filename = s3_hook.download_file(\n",
    "            bucket_name=bucket_name,\n",
    "            key=data_object_key)\n",
    "        print(f'download user event data to local: {data_object_key}, success!')\n",
    "        \"\"\"Calculates event statistics.\"\"\"\n",
    "        events = pd.read_json(temp_filename)\n",
    "        stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "        # 把結果寫到s3\n",
    "        with tempfile.NamedTemporaryFile('w+') as stats_file:\n",
    "            target_temp_filename = stats_file.name  # 暫存檔案名\n",
    "            stats.to_csv(target_temp_filename, index=False)\n",
    "            try:\n",
    "                s3_hook.load_file(filename=target_temp_filename,\n",
    "                                  bucket_name=bucket_name,\n",
    "                                  key=result_object_key,\n",
    "                                  replace=True)\n",
    "            except Exception as e:\n",
    "                raise AirflowException(e)\n",
    "    except Exception as e:\n",
    "        print(f\"download user event data from s3 fail:{e}\")\n",
    "        raise AirflowException(e)\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner':'EMPLOYEE_ID', # owner是DAG的開發者, 例如: 員工8703147\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"deXX_03_every3days\", # prefix必需是tenant id, 例如: de00\n",
    "    description=\"dag to calculate user events\",\n",
    "    start_date=days_ago(30),\n",
    "    schedule_interval=timedelta(days=3),\n",
    "    catchup=True, # 設定讓Airflow坐時光機去處理過去的數據管道\n",
    "    max_active_runs=1, # 設定同一個DAG最多只能有一個active的實例\n",
    "    default_args=default_args,\n",
    "    access_control={\n",
    "        'deXX': {'can_read', 'can_edit'} # 設定DAG歸屬那個團隊[tenant id]與權限\n",
    "    }\n",
    ")\n",
    "\n",
    "# task to retrieve rocket data and save to s3\n",
    "task_fetch_events = PythonOperator(\n",
    "    task_id='fetch_events',\n",
    "    python_callable=fetch_events,\n",
    "    op_kwargs={\n",
    "        'api_url': 'http://10.34.124.114:5000/events',\n",
    "        's3_conn_id': 'deXX_minio',\n",
    "        'bucket_name': 'EMPLOYEE_ID',\n",
    "        'object_key': 'de07/data/user_events.json'\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task to get rocket picture\n",
    "task_calculate_stats = PythonOperator(\n",
    "    task_id='calculate_stats',\n",
    "    python_callable=calculate_stats,\n",
    "    op_kwargs={\n",
    "        's3_conn_id': 'deXX_minio',\n",
    "        'bucket_name': 'EMPLOYEE_ID',\n",
    "        'data_object_key': 'de07/data/user_events.json',\n",
    "        'result_object_key': 'de07/data/stats.csv',\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task to notify someone when task is done\n",
    "task_notify = DummyOperator(task_id='notify', dag=dag)\n",
    "\n",
    "# Set dependencies between all tasks\n",
    "task_fetch_events >> task_calculate_stats >> task_notify\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
