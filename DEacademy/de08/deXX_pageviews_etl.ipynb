{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "import datetime as dt\n",
    "import requests\n",
    "from airflow.exceptions import AirflowException\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# download wikipedia pageviews data\n",
    "def get_data(s3_conn_id, bucket_name, object_key_prefix, **context):\n",
    "    year, month, day, hour, *_ = context['execution_date'].timetuple()\n",
    "    url = (\n",
    "        \"http://10.34.124.114:7080/other/pageviews/\"\n",
    "        f\"{year}/{year}-{month:0>2}/\"\n",
    "        f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz\"\n",
    "    )\n",
    "    print(url) # 打印URL來確認是否符合預期\n",
    "    # download wiki pageviews\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        raise AirflowException(f\"wikipedia pageviews data download fail, the resp status[{resp.status_code}]\")\n",
    "    # 透過Connection來取得S3Hook物件的實例\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # write rocket launch data to s3\n",
    "    with tempfile.NamedTemporaryFile('wb+') as fp:\n",
    "        temp_filename = fp.name  # 暫存檔案名\n",
    "        # use the same file hierarchy pattern\n",
    "        object_key = (\n",
    "            f\"{object_key_prefix}/task:get_data/\"\n",
    "            f\"{year}/{year}-{month:0>2}/\"\n",
    "            f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz\"\n",
    "        )\n",
    "        try:\n",
    "            fp.write(resp.content)\n",
    "            fp.flush()\n",
    "            s3_hook.load_file(filename=temp_filename,\n",
    "                              bucket_name=bucket_name,\n",
    "                              key=object_key,\n",
    "                              replace=True)\n",
    "            print(f'put wikipedia pageviews data to s3: [{bucket_name}] -> {object_key}, success!')\n",
    "        except Exception as e:\n",
    "            raise AirflowException(f\"put wikipedia pageviews data to s3 fail:{e}\")\n",
    "\n",
    "# task to unzip and extract data\n",
    "def extract_data(s3_conn_id, bucket_name, object_key_prefix, **context):\n",
    "    # datetime elements for data partition\n",
    "    year, month, day, hour, *_ = context['execution_date'].timetuple()\n",
    "    # create s3 connection\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # use the same file hierarchy pattern\n",
    "    source_object_key = (\n",
    "        f\"{object_key_prefix}/task:get_data/\"\n",
    "        f\"{year}/{year}-{month:0>2}/\"\n",
    "        f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz\"\n",
    "    )\n",
    "    # download s3 object to local temp file\n",
    "    try:\n",
    "        # 暫存檔案名\n",
    "        source_temp_filename = s3_hook.download_file(bucket_name=bucket_name,key=source_object_key)\n",
    "        print(f'download wikipedia pageviews data to local: {source_temp_filename}, success!')\n",
    "        # read wiki pageviews to dataframe\n",
    "        print(f'read and extract:{source_temp_filename}!')\n",
    "        df = pd.read_csv(source_temp_filename,\n",
    "                         names=['domain', 'title', 'view_count', 'response_size'],\n",
    "                         compression='gzip',\n",
    "                         delimiter=' ')\n",
    "        df2 = df[\n",
    "            (df['domain'] == 'en') &\n",
    "            (df['title'].isin([\"Google\", \"Amazon\", \"Apple\", \"Microsoft\", \"Facebook\"]))\n",
    "            ]\n",
    "        print(df2.info())\n",
    "        print(df2.head())\n",
    "        # remove temp file\n",
    "        os.remove(source_temp_filename)\n",
    "        # use the same file hierarchy pattern\n",
    "        target_object_key = (\n",
    "            f\"{object_key_prefix}/task:extract_data/\"\n",
    "            f\"{year}/{year}-{month:0>2}/\"\n",
    "            f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.csv\"\n",
    "        )\n",
    "        # write dataframe as parquet to s3\n",
    "        with tempfile.NamedTemporaryFile('w+') as fp2:\n",
    "            target_temp_filename = fp2.name  # 暫存檔案名\n",
    "            try:\n",
    "                df2.to_csv(target_temp_filename, index=False)\n",
    "                s3_hook.load_file(filename=target_temp_filename,\n",
    "                                  bucket_name=bucket_name,\n",
    "                                  key=target_object_key,\n",
    "                                  replace=True)\n",
    "                print(f'upload wikipedia pageviews extract data to s3: {target_temp_filename}, success!')\n",
    "            except Exception as e:\n",
    "                raise AirflowException(f\"upload wikipedia pageviews data to s3 fail:{e}\")\n",
    "    except Exception as e:\n",
    "        raise AirflowException(f\"download wikipedia pageviews data fail:{e}\")\n",
    "\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner':'EMPLOYEE_ID', # owner是DAG的開發者, 例如: 員工8703147\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"deXX_pageviews_etl\", # prefix必需是tenant id, 例如: de00\n",
    "    description=\"dag to download wikipedia pageviews\",\n",
    "    start_date=dt.datetime(2019,7,1),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    end_date=dt.datetime(2019,7,2),\n",
    "    catchup=True,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    access_control={\n",
    "        'deXX': {'can_read', 'can_edit'} # 設定DAG歸屬那個團隊[tenant id]與權限\n",
    "    },\n",
    "    tags=['de08'],\n",
    ")\n",
    "\n",
    "# task to download wikipedia pageviews data\n",
    "task_get_data = PythonOperator(\n",
    "    task_id='get_data',\n",
    "    python_callable=get_data,\n",
    "    op_kwargs={\n",
    "        's3_conn_id': 'deXX_minio',\n",
    "        'bucket_name': 'EMPLOYEE_ID', # 有英文字元要轉為小寫\n",
    "        'object_key_prefix': 'de08'\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task to unzip and extract data\n",
    "task_extract_data = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    op_kwargs={\n",
    "            's3_conn_id': 'deXX_minio',\n",
    "            'bucket_name': 'EMPLOYEE_ID', # 有英文字元要轉為小寫\n",
    "            'object_key_prefix': 'de08',\n",
    "        },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_load_data = DummyOperator(\n",
    "    task_id='load_data',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Set dependencies between all tasks\n",
    "task_get_data >> task_extract_data >> task_load_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
