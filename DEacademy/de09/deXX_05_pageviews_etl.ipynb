{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ebeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "import datetime as dt\n",
    "import requests\n",
    "from airflow.exceptions import AirflowException\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# download wikipedia pageviews data\n",
    "def get_data(s3_conn_id, bucket_name, object_key_prefix, **context):\n",
    "    year, month, day, hour, *_ = context['execution_date'].timetuple()\n",
    "    url = (\n",
    "        \"http://10.34.124.114:7080/other/pageviews/\"\n",
    "        f\"{year}/{year}-{month:0>2}/\"\n",
    "        f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz\"\n",
    "    )\n",
    "    print(url) # 打印URL來確認是否符合預期\n",
    "    # download wiki pageviews\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        raise AirflowException(f\"wikipedia pageviews data download fail, the resp status[{resp.status_code}]\")\n",
    "    # 透過Connection來取得S3Hook物件的實例\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # write rocket launch data to s3\n",
    "    with tempfile.NamedTemporaryFile('wb+') as fp:\n",
    "        temp_filename = fp.name  # 暫存檔案名\n",
    "        # use the same file hierarchy pattern\n",
    "        object_key = (\n",
    "            f\"{object_key_prefix}/task:get_data/\"\n",
    "            f\"{year}/{year}-{month:0>2}/\"\n",
    "            f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.gz\"\n",
    "        )\n",
    "        try:\n",
    "            fp.write(resp.content)\n",
    "            fp.flush()\n",
    "            s3_hook.load_file(filename=temp_filename,\n",
    "                              bucket_name=bucket_name,\n",
    "                              key=object_key,\n",
    "                              replace=True)\n",
    "            print(f'put wikipedia pageviews data to s3: [{bucket_name}] -> {object_key}, success!')\n",
    "            # push key value via xcoms\n",
    "            context[\"task_instance\"].xcom_push(key=\"object_key\", value=object_key)\n",
    "        except Exception as e:\n",
    "            raise AirflowException(f\"put wikipedia pageviews data to s3 fail:{e}\")\n",
    "\n",
    "# task to unzip and extract data\n",
    "def extract_data(s3_conn_id, bucket_name, object_key_prefix, **context):\n",
    "    # datetime elements for data partition\n",
    "    year, month, day, hour, *_ = context['execution_date'].timetuple()\n",
    "    # create s3 connection\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # retrieve object key from xcoms\n",
    "    source_object_key = context[\"task_instance\"].xcom_pull(task_ids=\"get_data\", key=\"object_key\")\n",
    "    # download s3 object to local temp file\n",
    "    try:\n",
    "        # 暫存檔案名\n",
    "        source_temp_filename = s3_hook.download_file(bucket_name=bucket_name,key=source_object_key)\n",
    "        print(f'download wikipedia pageviews data to local: {source_temp_filename}, success!')\n",
    "        # read wiki pageviews to dataframe\n",
    "        print(f'read and extract:{source_temp_filename}!')\n",
    "        df = pd.read_csv(source_temp_filename,\n",
    "                         names=['domain', 'title', 'view_count', 'response_size'],\n",
    "                         compression='gzip',\n",
    "                         delimiter=' ')\n",
    "        df2 = df[\n",
    "            (df['domain'] == 'en') &\n",
    "            (df['title'].isin([\"Google\", \"Amazon\", \"Apple\", \"Microsoft\", \"Facebook\"]))\n",
    "            ]\n",
    "        print(df2.info())\n",
    "        print(df2.head())\n",
    "        # remove temp file\n",
    "        os.remove(source_temp_filename)\n",
    "        # use the same file hierarchy pattern\n",
    "        target_object_key = (\n",
    "            f\"{object_key_prefix}/task:extract_data/\"\n",
    "            f\"{year}/{year}-{month:0>2}/\"\n",
    "            f\"pageviews-{year}{month:0>2}{day:0>2}-{hour:0>2}0000.csv\"\n",
    "        )\n",
    "        # write dataframe as parquet to s3\n",
    "        with tempfile.NamedTemporaryFile('w+') as fp2:\n",
    "            target_temp_filename = fp2.name  # 暫存檔案名\n",
    "            try:\n",
    "                df2.to_csv(target_temp_filename, index=False)\n",
    "                s3_hook.load_file(filename=target_temp_filename,\n",
    "                                  bucket_name=bucket_name,\n",
    "                                  key=target_object_key,\n",
    "                                  replace=True)\n",
    "                print(f'upload wikipedia pageviews extract data to s3: {target_temp_filename}, success!')\n",
    "                # push key value via xcoms\n",
    "                context[\"task_instance\"].xcom_push(key=\"object_key\", value=target_object_key)\n",
    "            except Exception as e:\n",
    "                raise AirflowException(f\"upload wikipedia pageviews data to s3 fail:{e}\")\n",
    "    except Exception as e:\n",
    "        raise AirflowException(f\"download wikipedia pageviews data fail:{e}\")\n",
    "\n",
    "# task to load the data to database\n",
    "def load_data(s3_conn_id, bucket_name, object_key_prefix, pg_conn_id, table_name, **context):\n",
    "    # datetime elements for data partition\n",
    "    year, month, day, hour, *_ = context['execution_date'].timetuple()\n",
    "    # create s3 connection\n",
    "    s3_hook = S3Hook(aws_conn_id=s3_conn_id)\n",
    "    # retrieve object key from xcoms\n",
    "    source_object_key = context[\"task_instance\"].xcom_pull(task_ids=\"extract_data\", key=\"object_key\")\n",
    "    # download s3 object to local temp file\n",
    "    try:\n",
    "        # 暫存檔案名\n",
    "        source_temp_filename = s3_hook.download_file(bucket_name=bucket_name, key=source_object_key)\n",
    "        print(f'download wikipedia pageviews count data to local: {source_temp_filename}, success!')\n",
    "        # read wiki pageviews to dataframe\n",
    "        print(f'read and extract:{source_temp_filename}!')\n",
    "        # there are 4 columns: domain, title, view_count, response_size\n",
    "        df = pd.read_csv(source_temp_filename)\n",
    "        # keep columns: title & view_count\n",
    "        df_selected = df[[\"title\",\"view_count\"]]\n",
    "        # rename columns: title -> pagename, view_count -> pageviewcount,\n",
    "        df_renamed = df_selected.rename(columns={'title':'pagename', 'view_count':'pageviewcount'})\n",
    "        # add extra two columns: datetime, batch_id\n",
    "        df_renamed['datetime'] = context['execution_date']\n",
    "        batch_id = f'{year}{month:0>2}{day:0>2}-{hour:0>2}'\n",
    "        print(f'batch_id: {batch_id}')\n",
    "        df_renamed['batch_id'] = batch_id\n",
    "        print(df_renamed.info())\n",
    "        print(df_renamed.head())\n",
    "        # create postgres connection\n",
    "        pg_hook = PostgresHook(postgres_conn_id=pg_conn_id)\n",
    "        # delete records for the same batch_id (so we can run this task again and again)\n",
    "        pg_hook.run(f\"DELETE FROM {table_name} WHERE batch_id='{batch_id}';\")\n",
    "        # get sqlalchemy engine instance\n",
    "        sqlalchemy_engine = pg_hook.get_sqlalchemy_engine()\n",
    "        try:\n",
    "            # dump dataframe to sql table via pandas.to_sql() method\n",
    "            df_renamed.to_sql(table_name, sqlalchemy_engine,\n",
    "                              if_exists='append', index=False, chunksize=5000)\n",
    "        except Exception as e:\n",
    "            raise AirflowException(f\"dump dataframe to table:{table_name} fail:{e}\")\n",
    "    except Exception as e:\n",
    "        raise AirflowException(f\"download wikipedia pageviews count data fail:{e}\")\n",
    "\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner':'EMPLOYEE_ID', # owner是DAG的開發者, 例如: 員工8703147\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"deXX_05_pageviews_etl\", # prefix必需是tenant id, 例如: de00\n",
    "    description=\"dag to download wikipedia pageviews\",\n",
    "    start_date=dt.datetime(2019,7,1,0),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    end_date=dt.datetime(2019,7,1,3),\n",
    "    catchup=True,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    access_control={\n",
    "        'deXX': {'can_read', 'can_edit'} # 設定DAG歸屬那個團隊[tenant id]與權限\n",
    "    },\n",
    "    tags=['de08'],\n",
    ")\n",
    "\n",
    "# task to download wikipedia pageviews data\n",
    "task_get_data = PythonOperator(\n",
    "    task_id='get_data',\n",
    "    python_callable=get_data,\n",
    "    op_kwargs={\n",
    "        's3_conn_id': 'deXX_minio',\n",
    "        'bucket_name': 'EMPLOYEE_ID', # 有英文字元要轉為小寫\n",
    "        'object_key_prefix': 'de08'\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task to unzip and extract data\n",
    "task_extract_data = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    op_kwargs={\n",
    "            's3_conn_id': 'deXX_minio',\n",
    "            'bucket_name': 'EMPLOYEE_ID', # 有英文字元要轉為小寫\n",
    "            'object_key_prefix': 'de08',\n",
    "        },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task to load pageview data into postgres db\n",
    "task_load_data = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=load_data,\n",
    "    op_kwargs={\n",
    "            's3_conn_id': 'deXX_minio',\n",
    "            'bucket_name': 'EMPLOYEE_ID',\n",
    "            'object_key_prefix': 'de08',\n",
    "            'pg_conn_id': 'deXX_postgres',\n",
    "            'table_name': 'pageview_counts_staging'\n",
    "        },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "# Set dependencies between all tasks\n",
    "task_get_data >> task_extract_data >> task_load_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
