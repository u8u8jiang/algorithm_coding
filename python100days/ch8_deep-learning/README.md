
# ch8 深度學習    

深度學習是加深層數的多層神經網路, 深度學習的性質、課題及其可能性.   

## 8.1 增加網路層數   
目前已學過: 構成神經網路各種層級, 助於學習的技巧, 能有效運用於影像辨識的CNN, 參數最佳化手法.   
NOW: 製作多層網路, 挑戰MNIST資料集的手寫數字辨識.    

以VGG網路為代表    
* 3*3的小濾鏡形成卷積層 -> 色版數量隨著層數加深而變大, 插入池化層的中間資料空間逐漸變小   
* 活化函數為ReLU    
* 全連接層後使用dropout層    
* 使用「He預設值」當作權重預設值, 並以Adam更新權重參數 最佳化    

> deep_convnet.py       
* class DeepConvNet    
* 可載入學過的參數 deep_convnet_params.pkl   

> train_deepnet.py  
* network = DeepConvNet()  

## 進一步提高辨識準確度   
如: 整體學習, 學習率衰減(learning rate decay), 資料擴增(data argumentation)...    
* 資料擴增(data argumentation):     
    * 變形-利用旋轉、垂直或水平移動等方式, 小幅改變輸入影像, 增加影像張數.     
    * 擷取影像中的某一部分的「crop處理」, 或左右翻轉的「flip處理」    
    * 還可以改變明度等外觀或縮放大小     
* 加深層數, 能以較少的參數達到相同(或更好)的表現力 - since卷積運算的濾鏡大小...           
    * 可以反映出邊界、紋理、物體部位, 依照階層逐漸變複雜     
    * 階層性傳遞資料, 把各層需學習的問題分解成容易解決的簡單問題.   


## 8.2 深度學習的歷史    
keywords: ILSVRC, ImageNet    
- 2012年起, 深度學習手法經常獲得第一名, AlexNet錯誤率大幅降低    
- 2015 ResNet(超過150層的網路), 錯誤辨識率降至3.5%        

VGG, GoogleNet, ResNet...   
- VGG: 由3*3小濾鏡連接卷積層, 連續2~4次的卷積層, 重複進行用池化層把大小變成一半的處理, 最後經由全連接層輸出結果. (2014年的第二名, 當年優勝為GoogleNet)     
    * VGG結構簡單, 應用性高   
    * GoogleNet不僅會往垂直方向加深網路, 也會往水平方向加深(增廣) - Inception結構, 套用多個大小不同的濾鏡(與池化), 再整合結果, 在多個位置使用1*1濾鏡的卷積層, 藉由縮小色版尺寸, 減少參數及提高處理速度.           
- ResNet: Miscrosoft團隊所開發, 加深了結構     
    * 加深過多結構導致無法順利學習, 導入跳躍結構...跳過輸入資料的卷積層, 加總輸出的結構.    

:elephant: 學習遷移- 常把ImageNet的龐大資料集, 學習到的權重資料運用其他情況, 把學習完畢的權重當作預設值, 以新資料集為對象, 進行fine tuning.   



## 8.3 深度學習的高速化- GPU, 分散式學習, 縮減運算精確度的位元      
* 近期的深度學習框架可支援多個GPU與多台裝置進行分散式學習     
* 深度學習中哪些處理較花時間? 花費較多的時間在卷積層, 有效提升在 卷積層的運算(積和運算) 為深度學習的重要課題.      
* GPU 把強大的效能運用在各種用途: 圖形專用處理器, 快速處理數值的平行運算      
    * keyword: cuDNN, NVIDIA & AMD  
    * :star: 大部分深度學習框架使用 NVIDIA 提供的GPU運算整合開發環境 CUDA, 而cuDNN是在CUDA上執行的函式庫, 其中包含深度學習常用的最佳化函數.    
    * 卷積層運算, 可以利用im2col轉換成大型陣列的乘積 ->GPU擅長一次完成大型運算      
    * 分散式學習, 橫向擴展深度學習的學習階段, 使用多個GPU或多台裝置提升運算速度. 其中, Google的tensorflow, Microsoft的CNTK, 即為針對分散式學習開發.            
* :snail: 包括運算量在內, 記憶體容量、匯流排頻寬等皆會造成瓶頸.   
    * 就記憶體容量, 大量權重參數、中間資料 皆會存於記憶體中     
    * 匯流排頻寬, 一旦通過匯流排資料超過一定限制....    
    * 縮減運算精確度的位元      

## 8.4 深度學習的應用範例     
* 物件偵測(處理手法如:R-CNN): 分析物體種類及物體位置, 主要由兩處理構成, 截取候選區域與CNN兩部分.      
* 影像分割(處理手法如:FCN): 以像素標籤進行類別分類.      
* 產生圖說(處理手法如:NIC): 由處理多層的CNN, 與自然語言的RNN所構成. RNN常用於自然語言、時間序列資料等連續資料上.         

## 8.5 深度學習的未來       
* 轉換影像風格: 輸入內容影像與風格影像, 產生新影像. 輸入影像模擬內容影像的形狀, 從風格影像吸收風格(風格陣列).     
* 產生影像(處理手法如:DCGAN): 從零開始產生新影像. 運用generator產生與本尊相似的影像, discriminator判斷是否為本尊, 最終畫出與本尊一模一樣的影像.     
* 自動駕駛: 路線規劃, 攝影機或雷射等感應技術, 識別周圍環境(處理手法如:SegNet).  
* 強化學習(Deep Q-Network): 嘗試錯誤的過程, 代理人根據環境狀況決定要採取的行動, 隨著環境變化獲得更好的報酬. - AlphaGo, DQN is created by Google Deep Mind.      




